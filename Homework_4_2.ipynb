{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <h1 style=\"margin-top: 50px; font-size: 33px; text-align: center\"> Homework 4 </h1>\n",
    "    <br>\n",
    "    <div style=\"font-weight:200; font-size: 20px; padding-bottom: 15px; width: 100%; text-align: center;\">\n",
    "        <right>Fabio Montello, Maria Luisa Croci, Eltaj Babanli</right>\n",
    "        <br>\n",
    "    </div>\n",
    "    <hr>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <h1 style=\"margin-top: -5px; font-size: 20px; text-align: center\"> 2) Find the duplicates! </h1>\n",
    "    <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting from passwords2.txt file as input with each row having a string of 20 characters we want to define a hash function that associates a value to each string. \n",
    "The goal is to *check whether there are some duplicate strings*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first part we define duplicate when two strings have the same characters, order is not important. Thus, \"AABA\" = \"AAAB\", and we count it as *one duplicate*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the amount of data is very large, we decided to use Hadoop Spark to process all the data. This choice will let us use the full potential on the CPU enabling the use of multicore and increasing the speed of execution. It will also reduce the amount of RAM needed for the esecution.\n",
    "We start by importing the `pyspark` environment, which is an interface to use Spark on python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init()\n",
    "sc = pyspark.SparkContext(appName=\"findDuplicates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.134:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>findDuplicates</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=findDuplicates>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we proceed by telling to the Spark environment which file text take as input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = sc.textFile(\"data/passwords2.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's not see if the data gets properly imported by printing the first 5 number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OHcv-/U3QI$rdqYTef\"D',\n",
       " 'QtA*.xM$e(+\"aO36r&Uo',\n",
       " \"T;rqw/ou'HN-Pklj6hM*\",\n",
       " 'b%xJ79\"A*C5@ehMfS3lu',\n",
       " 'buI=;LpjBiCm\"JS5\\'#xy']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we want to define the function that generates the hash function from the string. A hash function takes an item of a given type and generates an integer hash value within a given range. The input items can be anything: strings, compiled shader programs, files, even directories. The same input always generates the same hash value, and a good hash function tends to generate different hash values when given different inputs. A hash function has no awareness of “other” items in the set of inputs. It just performs some arithmetic and/or bit-magic operations on the input item passed to it. Therefore, there’s always a chance that two different inputs will generate the same hash value [[1]](https://preshing.com/20110504/hash-collision-probabilities/).\n",
    "\n",
    "Now let's build the function that will generate the first hash function. Since we do not need to encrypt the data, but just count the duplicates, we will not focus on encoding our hash, but just generating a reasonable code that won't take too much computational time or space in memory. In any case we want to mention that at this [link](http://www.metamorphosite.com/one-way-hash-encryption-sha1-data-software) there is an interesting example on how to use the XOR operator to make an hash function encrypted (which is the real purpose of hash functions).\n",
    "\n",
    "Our hash function is so composed: we extract the ASCII code of every single element in the string and we sum the power to the cube of this number with all the number computed previously. \n",
    "In this way we use the commutative property to not consider the actual position of the elements. Furthermore we power the number to the cube to improve the differentation between numbers and try to avoid as much as possible the collision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hashingMap(item):\n",
    "    s = 0\n",
    "    for c in item:\n",
    "        s += (ord(c)**3)\n",
    "    return (s, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to compute the map and reduce functions. The reduce function is written inline, since it consists of just a counter of how many times the elements with the same keys repeats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntuples = txt.map(hashingMap).reduceByKey(lambda a, b: a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(14206962, 17), (9652134, 14), (13829394, 22), (13533246, 20), (10688514, 15)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ntuples.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced = ntuples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to filter now all the elements that does not have duplicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'reduced' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-cb1976afc287>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mduplicates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduced\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'reduced' is not defined"
     ]
    }
   ],
   "source": [
    "duplicates = reduced.filter(lambda x: x[1]>1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(14206962, 17),\n",
       " (9652134, 14),\n",
       " (13829394, 22),\n",
       " (13533246, 20),\n",
       " (10688514, 15),\n",
       " (9515031, 17),\n",
       " (11009847, 24),\n",
       " (14349378, 13),\n",
       " (12953232, 23),\n",
       " (12682890, 22)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicates.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally we want to count the number of duplicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9448196"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = duplicates.count()\n",
    "n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we got 9448196 duplicates, which is very close to the 10000000 duplicates expected. Increasing the power to the 4 or 5 would for sure improve the quality of the algorithm avoiding collision better. At the same time it would slow down the code. That's why we think that the power to the cube is the right threshold in between efficiency and collision probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the second part we define duplicate when two strings have the same characters and order is also important. Thus, \"AABA\" is not equals \"AAAB\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to generate a function where position matters. So we decided to elaborate the data in a peculiar way. We started by composing a string that has the binary code of the whole password we have to transform. Then we wanted to unify the strings, so we added zeros where the string was not long enough, and removed characters when it was too long. In this way we controlled the length of the int variable we are going to output. In our case we decided to keep 64 bit as length of our integer, a reasonable value to not take too much space in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hashingMapUnique(item):\n",
    "    s = ''\n",
    "    for c in item:\n",
    "        s += \"{0:b}\".format(ord(c))\n",
    "    return (int(s.ljust(64, '0')[:64:],2), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we do the map-reduce procedure to count the data, as we did previously, filtering just the duplicates and counting them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntuples = txt.map(hashingMapUnique).reduceByKey(lambda a, b: a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(16274527251664803781, 2),\n",
       " (10608099667156110106, 2),\n",
       " (16668620600776809073, 2),\n",
       " (10081794263021956144, 2),\n",
       " (13073833392952119731, 2)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ntuples.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced = ntuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = reduced.filter(lambda x: x[1]>1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(16274527251664803781, 2),\n",
       " (10608099667156110106, 2),\n",
       " (16668620600776809073, 2),\n",
       " (10081794263021956144, 2),\n",
       " (13073833392952119731, 2),\n",
       " (10014301253483687536, 2),\n",
       " (12539534125667135270, 2),\n",
       " (17272142619530314309, 2),\n",
       " (13455788835654306926, 2),\n",
       " (15110295247321585083, 2)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicates.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = duplicates.count()\n",
    "n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As result we can see that the number of duplicates in precisely 5000000. This means that the choice of taking 64 bits was reasonable to keep the data not too large and at the same time have a very good precision on the counting, avoiding at all the hash collision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
